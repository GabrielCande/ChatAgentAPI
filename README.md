# Chat Agent API

Uma API inteligente de chat construÃ­da com **FastAPI** e **Strands Agents**, capaz de realizar **cÃ¡lculos matemÃ¡ticos** e responder a **perguntas gerais** utilizando modelo de IA local via **Ollama**. Foi tambÃ©m implementada uma web UI integrada, com tratamento de resposta da API, para melhor visualizaÃ§Ã£o de seu funcionamento.

## SumÃ¡rio

- [IntroduÃ§Ã£o](#introduÃ§Ã£o)
- [Requisitos](#requisitos)
- [InstruÃ§Ãµes de ExecuÃ§Ã£o](#instruÃ§Ãµes-de-execuÃ§Ã£o)
- [DocumentaÃ§Ã£o](#documentaÃ§Ã£o)
- [LicenÃ§a](#licenÃ§a)

## IntroduÃ§Ã£o

A API Ã© capaz de:

- **Responder perguntas gerais** usando conhecimento do modelo de linguagem.
- **Realizar cÃ¡lculos matemÃ¡ticos** complexos atravÃ©s de _tools_ especializadas.
- **Decidir automaticamente** quando usar cÃ¡lculos vs. conhecimento geral.
- **Executar localmente** sem dependÃªncia de serviÃ§os cloud.

# Requisitos

Esta seÃ§Ã£o detalha os requisitos de software, modelo de linguagem e hardware necessÃ¡rios para executar a Chat Agent API.

### Software NecessÃ¡rio

| Componente | VersÃ£o | Link                                                 |
| :--------- | :----- | :--------------------------------------------------- |
| **Python** | 3.10+  | [Download Python](https://www.python.org/downloads/) |
| **Ollama** | Latest | [Download Ollama](https://ollama.ai/download)        |

### Modelo de Linguagem

O projeto utiliza um modelo de linguagem local via Ollama:

| Modelo           | Tamanho | Download                  |
| :--------------- | :------ | :------------------------ |
| **Llama 3.1 8B** | ~4.7GB  | `ollama pull llama3.1:8b` |

### Hardware Recomendado

Para uma performance ideal, especialmente durante o carregamento do modelo:

- **RAM:** MÃ­nimo 8GB, recomendado **16GB**.
- **GPU:** Opcional, mas **recomendada** para melhor performance.
- **Armazenamento:** 10GB livres.

# InstruÃ§Ãµes de ExecuÃ§Ã£o

Siga os passos abaixo para configurar e executar a Chat Agent API.

### 1. Clone do RepositÃ³rio

Use o Git para clonar o projeto:

```bash
# VÃ¡ atÃ© o local onde deseja clonar o repositÃ³rio e execute:
git clone https://github.com/GabrielCande/ChatAgentAPI

```

### 2. InstalaÃ§Ã£o do Ollama

Baixe e execute o instalador em [ollama.ai/download](https://ollama.com/download).

Abra o prompt de comando (CMD) e realize o download do modelo llama3.1:8b:

```bash
# Utilize o comando pull
ollama pull llama3.1:8b

# Ou utilize o comando run que tambÃ©m irÃ¡ realizar o download
ollama run llama3.1:8b

```

### 3. ConfiguraÃ§Ã£o do Ambiente

No CMD navegue atÃ© a raiz do diretÃ³rio onde foi clonado o repositÃ³rio:

```bash
cd C:\Gabriel\Github\ChatAgentAPI # (apenas um exemplo de path)

```

Em seguida inicialize o ambiente:

```bash
# Cria o ambiente
python -m venv venv

# Ativa o ambiente
venv\Scripts\activate

```

ApÃ³s a incializaÃ§Ã£o do ambiente instale as bibliotecas necessÃ¡rias:

```bash
# Dentro do ambiente ativo execute:
pip install -r requirements.txt

```

Por fim, crie um arquivo denominado ".env" na raiz do diretÃ³rio do repositÃ³rio clonado, manualmente ou pelo cÃ³digo seguinte no terminal:

```bash
# (apenas um exemplo de path)
cd C:\Gabriel\Github\ChatAgentAPI

# Cria arquivo .env
type nul > .env

```

Coloque neste arquivo a seguinte configuraÃ§Ã£o:

```bash
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL=llama3.1:8b

```

### 4. InÃ­cio do Ollama

Abra um novo CMD para realizar a inicializaÃ§Ã£o da execuÃ§Ã£o do modelo llama3.1:8b instalado:

```bash
ollama serve

```

OBS.:

1. Esse CMD deverÃ¡ continuar aberto, nÃ£o o feche;

2. Caso o comando nÃ£o funcione certifique-se de que o Ollama nÃ£o estÃ¡ aberto ou em execuÃ§Ã£o.

### 5. ExecuÃ§Ã£o da API

Volte para o CMD onde vocÃª estÃ¡ no ambiente criado dentro da raiz do diretÃ³rio do repositÃ³rio e execute os comandos:

```bash
# Em "(venv) C:\seu path\ChatAgentAPI>" execute:
# Para acessar a pasta src
cd src

# Para inicializar a API
python main.py

```

### 6. Realizando testes

Para testar o funcionamento da API basta acessar, atravÃ©s de seu browser, a web UI integrada:

```bash
# URL da web UI:
http://localhost:8000/webui/

```

---

Caso queira verificar a raw response gerada Ã© possÃ­vel de se fazer via a documentaÃ§Ã£o interativa automÃ¡tica da API, gerada pelo FastAPI usando Swagger UI:

```bash
# URL da documentaÃ§Ã£o interativa:
http://localhost:8000/docs

```

Para testar utilizando essa interface basta seguir os passos:

- Clique na seÃ§Ã£o em verde "POST /chat Chat Endpoint" para expandi-la
- Em seguida clique no botÃ£o "Try it out" (primeiro botÃ£o a direita)
- Depois de precionar o botÃ£o vocÃª verÃ¡ algo do tipo:

```bash
{
  "message": "string"
}

```

- Altere o conteÃºdo de "string" para a mensagem que deseja enviar, exemplo:

```bash
{
  "message": "OlÃ¡, tudo bem?"
}

```

- EntÃ£o precione o botÃ£o em azul "Execute"
- ApÃ³s o carregamento a resposta serÃ¡ gerada e mostrada um pouco mais abaixo, vocÃª verÃ¡ algo do tipo:

```bash
Response body
{
  "response": "Tudo bem! Estou aqui para ajudar. Como posso ajudar vocÃª hoje?\n"
}

```

---

Ã‰ possÃ­vel tambÃ©m testar diretamente via terminal, basta abrir um novo terminal (alÃ©m dos 2 necessÃ¡rios para rodar a API) e digitar o seguinte comando:

```bash
# Basta trocar o campo SUA MENSAGEM AQUI pela mensagem desejada:
curl -X POST http://localhost:8000/chat -H "Content-Type: application/json" -d "{\"message\": \"SUA MENSAGEM AQUI\"}"

```

OBS.: Ã‰ possÃ­vel verificar atravÃ©s do terminal que estÃ¡ rodando a "main.py" todas as respostas das requisiÃ§Ãµes realizadas em tempo real, indepenedente do local e mÃ©todo que utilizou para realiza-las (webUI, fastAPI docs ou terminal).

# DocumentaÃ§Ã£o

Links para as documentaÃ§Ãµes:

| Recurso            | DocumentaÃ§Ã£o                                                                 |
| :----------------- | :--------------------------------------------------------------------------- |
| **Strands Agents** | [DocumentaÃ§Ã£o Oficial](https://strandsagents.com/latest/documentation/docs/) |
| **FastAPI**        | [FastAPI Docs](https://fastapi.tiangolo.com/)                                |
| **Ollama**         | [Ollama Docs](https://github.com/ollama/ollama)                              |
| **Llama 3.1**      | [Model Card](https://ollama.com/library/llama3.1)                            |
| **Uvicorn**        | [Uvicorn Docs](https://uvicorn.dev/)                                         |

Estrutura do projeto:

```bash
ChatAgentAPI/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ main.py               # AplicaÃ§Ã£o FastAPI principal
â”‚   â”œâ”€â”€ agent.py              # Agente de IA
â”‚   â”œâ”€â”€ ğŸ“ tools/
â”‚   â”‚    â””â”€â”€ mathTool.py      # Tool especializada em cÃ¡lculos
â”‚   â””â”€â”€ ğŸ“ webUI/
â”‚        â”œâ”€â”€ index.html       # PÃ¡gina HTML da interface web
â”‚        â”œâ”€â”€ ğŸ“ css/
â”‚        â”‚    â””â”€â”€ styles.css  # Estilo css
â”‚        â””â”€â”€ ğŸ“ js/
â”‚             â””â”€â”€ app.js      # JavaScript da interface web
â”œâ”€â”€ requirements.txt          # DependÃªncias do projeto
â”œâ”€â”€ .env                      # VariÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```

# LicenÃ§a

Este projeto Ã© open-source e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.
